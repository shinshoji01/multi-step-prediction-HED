{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "base_dir = \"./audio/\"\n",
    "# base_dir = \"./audiosamples/\"\n",
    "# other_dir = \"../seq2seq-vc/datasetgeneration/LLM_responses/08-Others_multi-lingual_text/\"\n",
    "speakers = [str(i).zfill(4) for i in range(11, 21)]\n",
    "\n",
    "def read_file(path):\n",
    "    encodings = ['utf-8', 'utf-16', 'iso-8859-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(path, 'r', encoding=encoding) as file:\n",
    "                return file.readlines(), encoding\n",
    "        except (UnicodeError, UnicodeDecodeError) as e:\n",
    "            continue\n",
    "    raise Exception(\"Failed to decode file with any of the specified encodings.\")\n",
    "    \n",
    "transcription_dir = \"./audio/text_dir_LibriTTS.npy\"\n",
    "transcriptions = np.load(transcription_dir, allow_pickle=True).item()\n",
    "\n",
    "transcription_dir = \"../Dataset/ESD/\"\n",
    "emotions = {}\n",
    "for spk in speakers:\n",
    "    path = transcription_dir+spk+f\"/{spk}.txt\"\n",
    "    try:\n",
    "        text, used_encoding = read_file(path)\n",
    "        print(f\"File read successfully with encoding: {used_encoding}\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    text, used_encoding = read_file(path)\n",
    "                \n",
    "    for txt in text:\n",
    "        if len(txt.split())<2:\n",
    "            continue\n",
    "        try:\n",
    "            fn, tcp, emotion = txt[:-1].split(\"\\t\")\n",
    "        except ValueError:\n",
    "            fn = txt.split(\" \")[0]\n",
    "            txt = \" \".join(txt.split(\" \")[1:])\n",
    "            tcp, emotion = txt[:-1].split(\"\\t\")\n",
    "        \n",
    "        transcriptions[fn] = tcp\n",
    "        emotions[fn] = emotion\n",
    "        \n",
    "def wavfiletext(wavfile):\n",
    "    a = f\"\"\"\n",
    "                      <td>\n",
    "    \"\"\"[1:]\n",
    "    a += f\"\"\"\n",
    "                        <audio controls=\"\" preload=\"none\" style=\"width: 240px\">\n",
    "                          <source src=\"{wavfile}\" type=\"audio/wav\">\n",
    "                        </audio>\n",
    "    \"\"\"[1:]\n",
    "    a += f\"\"\"\n",
    "                      </td>\n",
    "    \"\"\"[1:]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "webtitle = \"Multi-Step Prediction of Hierarchical Emotion Distribution Demo Page\"\n",
    "title = \"Prediction and Control of Hierarchical Emotion Distribution for Text-to-Speech Synthesis\"\n",
    "abstract = \"This paper extends our previous work by investigating hierarchical emotion distribution (ED) for achieving multi-level quantitative control of emotion rendering in text-to-speech synthesis (TTS). We introduce a novel multi-step hierarchical ED prediction module that quantifies emotion variance at the utterance, word, and phoneme levels. By predicting emotion variance in a multi-step manner, our TTS framework leverages global emotional context to refine local emotional variations, thereby capturing the intrinsic hierarchical structure of speech emotion.  Our approach is validated through its integration into a variance adaptor and an external module design compatible with various TTS systems.  Both objective and subjective evaluations demonstrate that our framework significantly enhances emotional expressiveness and enables precise control of emotion rendering across multiple speech granularities.\"\n",
    "github_url = \"https://github.com/shinshoji01/multi-step-prediction-HED-project-page\"\n",
    "# base_repo_dir = \"/\"\n",
    "base_repo_dir = \"/multi-step-prediction-HED/\"\n",
    "style_dir = base_repo_dir + \"statics/\"\n",
    "fig_path = base_repo_dir + \"images/emotion_intensity.png\"\n",
    "\n",
    "initial = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n",
    "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n",
    "    <title>{webtitle}</title>\n",
    "    <link href=\"{style_dir}bootstrap-5.2.3-dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    <link href=\"{style_dir}my.css\" rel=\"stylesheet\">\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"container\">\n",
    "      <div class=\"row\">\n",
    "        <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "          <div class=\"text-center\">\n",
    "            <h2>{title}</h2>\n",
    "            <br>\n",
    "            <h5 class=\"mb-4\">Sho Inoue<sup>1,2</sup>, Kun Zhou<sup>3</sup>, Shuai Wang<sup>2†</sup>, Haizhou Li<sup>1,2</sup></h5>\n",
    "              <p>\n",
    "                <sup>1</sup>School of Data Science, <sup>2</sup>Shenzhen Research Institute of Big Data<br>\n",
    "                The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), Shenzhen, China<br>\n",
    "                <sup>3</sup>Alibaba Group, Singapore\n",
    "              </p>\n",
    "          </div>\n",
    "          <br>\n",
    "          <figure class=\"text-center\">\n",
    "            <img src=\"{fig_path}\" alt=\"overall diagram of the pipeline\" class=\"img-fluid\" style=\"width: 900px; height: auto;\">\n",
    "          </figure>\n",
    "          <br>\n",
    "          <h3>Abstract</h3>\n",
    "          <p class=\"lead\">\n",
    "          {abstract}\n",
    "        </div>\n",
    "\"\"\"[1:]\n",
    "\n",
    "closure = f\"\"\"\n",
    "      </div>\n",
    "    </div>\n",
    "    <script src=\"{style_dir}jquery/jquery-3.7.1.slim.min.js\"></script>\n",
    "    <script src=\"{style_dir}bootstrap-5.2.3-dist/bootstrap.min.js\"></script>\n",
    "\"\"\"[1:]\n",
    "closure += \"\"\"\n",
    "  </body>\n",
    "  <script>\n",
    "    $(function(){\n",
    "        $(\"audio\").on(\"play\", function() {\n",
    "            $(\"audio\").not(this).each(function(index, audio) {\n",
    "                audio.pause();\n",
    "                audio.currentTime = 0;\n",
    "            });\n",
    "        });\n",
    "    });\n",
    "    </script>\n",
    "</html>\n",
    "\"\"\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [p for p in transcriptions if len(p.split(\"_\"))>=3]\n",
    "filenames.sort()\n",
    "np.random.seed(0)\n",
    "filenames = list(np.random.choice(filenames, 20, replace=False))\n",
    "# filenames.sort()\n",
    "# controlfns = [\"0014_000732\", \"0015_000726\", \"0013_000731\", \"0018_000037\"]\n",
    "controlfns = [\"0015_000011\", \"0014_000354\", \"0018_000009\", \"0020_000018\"]\n",
    "\n",
    "emos = [\"Angry\", \"Happy\", \"Sad\", \"Surprise\"]\n",
    "intensities = [0.0, 0.4, 0.6, 1.0]\n",
    "emotioncolors = {\n",
    "    \"Angry\": \"red\",\n",
    "    \"Happy\": \"orange\",\n",
    "    \"Sad\": \"blue\",\n",
    "    \"Surprise\": \"green\",\n",
    "}\n",
    "modifiedparts = {\n",
    "\"Who is been repeating all that hard stuff to you?\": \"Who is been <u>repeating</u> all that <u>hard</u> <u>stuff</u> to you?\", \n",
    "\"Let's make the noise a snake.\": \"<u>Let's</u> make the <u>noise</u> a <u>snake</u>.\", \n",
    "\"All smile were real and the happier，the more sincere .\": \"All <u>smile</u> were real and the <u>happier</u>，the more <u>sincere</u> .\", \n",
    "\"I think it'll encourage me.\": \"I <u>think</u> <u>it'll</u> <u>encourage</u> me.\",\n",
    "\"Chapter ten a warm welcome.\": \"<u>Chapter</u> ten a <u>warm</u> <u>welcome</u>.\",\n",
    "\"They went up to the dark mass job had pointed out.\": \"They <u>went</u> up to the <u>dark</u> mass job had <u>pointed</u> out.\",\n",
    "\"On the twenty second of last march.\": \"On the <u>twenty</u> <u>second</u> of last <u>march</u>.\",\n",
    "\"Then sadly it is much farther.\": \"Then <u>sadly</u> it is <u>much</u> <u>farther</u>.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\n",
    "    \"expressiveness\": \"Section 1: Emotion Expressiveness: Reproducibility via Ground-Truth Emotion Information (Emotion Transfer)\",\n",
    "    \"utterance\": \"Section 2: Utterance-level Emotion Intensity Control\",\n",
    "    \"word\": \"Section 3: Word-level Emotion Intensity Control\",\n",
    "}\n",
    "explanations = {\n",
    "    \"expressiveness\": \"\\\n",
    "The first section presents a demo to evaluate our model's reproducibility. The Hierarchical EDs are obtained by the reference audio for those models with 'GT'. Instead, the Hierarchical EDs are predicted by textual cues for those with 'Pred'. 'External' and 'VA' denote different integration methods for ED modeling within TTS frameworks. In the 'External' approach, we employ a model-agnostic pipeline and incorporate a hierarchical ED embedding following text processing. In contrast, the 'VA' approach integrates hierarchical ED modeling directly within the variance adaptor of FastSpeech2: \\\n",
    "<ul> \\\n",
    "  <li><b>Reference Speech</b>: The reference audio<br></li> \\\n",
    "  <li><b>GT | External</b>: 'External' Model with Ground-Truth Hierarchical ED<br></li> \\\n",
    "  <li><b>GT | VA (Baseline)</b>: 'VA' Model with Ground-Truth Hierarchical ED<br></li> \\\n",
    "  <li><b>GT | VA(Multi-Step)</b>: 'VA(Multi-Step)' Model with Ground-Truth Hierarchical ED<br></li> \\\n",
    "  <li><b>Pred | External | One-Step (Baseline)</b>: 'External' Model with Predicted Hierarchical ED from One-Step Predictor<br></li> \\\n",
    "  <li><b>Pred | External | Multi-Step</b>: 'External' Model with Predicted Hierarchical ED from Multi-Step Predictor<br></li> \\\n",
    "  <li><b>Pred | VA (Baseline)</b>: 'VA' Model with Predicted Hierarchical ED by One-Step Predictor<br></li> \\\n",
    "  <li><b>Pred | VA(Multi-Step)</b>: 'VA(Multi-Step)' Model with Predicted Hierarchical ED by Multi-Step Predictor<br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"utterance\": \"\\\n",
    "In this section, we demonstrate the control of utterance-level emotion intensities. We set the emotion intensities of the emotion to the values in the first row while setting the other emotion intensities 0.0. \\\n",
    "<ul> \\\n",
    "  <li><b>Ground Truth</b>: The Ground Truth sample reconstructed by Vocoder<br></li> \\\n",
    "  <li><b>External</b>: 'External' Model<br></li> \\\n",
    "  <li><b>VA(Multi-Step)</b>: 'VA(Multi-Step)' Model<br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"word\": \"\\\n",
    "In this section, we demonstrate the control of emotion intensities in a specific segment of speech, primarily focusing on <u>three words with underscores</u>. We set the word-level emotion intensities of these words, and the corresponding phoneme-level intensities, to the values in the first row, while maintaining the other emotion intensities constant. This showcases our model's ability to adjust emotion granularly within specified speech segments. We also present the speech samples focusing on the modified parts ('___OnlyModified___'). \\\n",
    "<ul> \\\n",
    "  <li><b>Ground Truth</b>: The Ground Truth sample reconstructed by Vocoder<br></li> \\\n",
    "  <li><b>External</b>: 'External' Model<br></li> \\\n",
    "  <li><b>VA(Multi-Step)</b>: 'VA(Multi-Step)' Model<br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "}\n",
    "filenames_dir = {\n",
    "    \"expressiveness\": filenames,\n",
    "    \"utterance\": controlfns,\n",
    "    \"word\": controlfns,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"expressiveness\": {\n",
    "        \"expressiveness/ground-truth\": \"Reference Speech\",\n",
    "        \n",
    "        \"expressiveness/External/gtHED\": \"GT | External\",\n",
    "        \"expressiveness/Variance-Adaptor/gtHED\": \"GT | VA\",\n",
    "        \"expressiveness/Variance-Adaptor_seq/gtHED\": \"GT | VA(Multi-Step)\",\n",
    "        \n",
    "        \"expressiveness/External/predHED---External---noseq\": \"Pred | External | One-Step\",\n",
    "        \"expressiveness/External/predHED---External\": \"Pred | External | Multi-Step\",\n",
    "        \n",
    "        \"expressiveness/Variance-Adaptor/predHED---Variance-Adaptor\": \"Pred | VA\",\n",
    "        \"expressiveness/Variance-Adaptor_seq/predHED---Variance-Adaptor_seq\": \"Pred | VA(Multi-Step)\",\n",
    "    },\n",
    "    \"utterance\": {\n",
    "        \"control/ground-truth\": \"Ground Truth\",\n",
    "        \"control/External_Embedding/utterance\": \"External\",\n",
    "        \"control/Variance-Adaptor_Embedding_seq/utterance\": \"VA(Multi-Step)\",\n",
    "    },\n",
    "    \"word\": {\n",
    "        \"control/ground-truth\": \"Ground Truth\",\n",
    "        \"control/External_Embedding/words-phonemes\": \"External\",\n",
    "        \"control/External_Embedding/words-phonemes---cut\": \"___OnlyModified___\",\n",
    "        \"control/Variance-Adaptor_Embedding_seq/words-phonemes\": \"VA(Multi-Step)\",\n",
    "        \"control/Variance-Adaptor_Embedding_seq/words-phonemes---cut\": \"___OnlyModified___\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfiles = []\n",
    "extexts = {}\n",
    "for exid in experiments:\n",
    "    text = f\"\"\"\n",
    "            <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "              <h2>{titles[exid]}</h2>\n",
    "              <p class=\"lead\">\n",
    "                    {explanations[exid]}\n",
    "                    </p>\n",
    "    \"\"\"[1:]\n",
    "    emotion_list = emos if exid in [\"utterance\", \"word\"] else [\"noemotion\"]\n",
    "    for emotion in emotion_list:\n",
    "        if exid in [\"utterance\", \"word\"]:\n",
    "            text += f\"\"\"\n",
    "                  <br><hr><br>\n",
    "                  <h4>{exid[0].upper()}{exid[1:]}-level Control: Emotion: <span style='color:{emotioncolors[emotion]};'>{emotion}</span></h4>\n",
    "                  <br>\n",
    "        \"\"\"[1:]\n",
    "        text += f\"\"\"\n",
    "                  <div class=\"table-responsive\" style=\"overflow-x: scroll\">\n",
    "                    <table class=\"table table-sm\">\n",
    "        \"\"\"[1:]\n",
    "        text += f\"\"\"\n",
    "                      <thead>\n",
    "                        <tr>\n",
    "                          <th scope=\"col\">ID</th>\n",
    "        \"\"\"[1:]\n",
    "        for fn in filenames_dir[exid]:\n",
    "            if exid==\"expressiveness\":\n",
    "                repeat = 1\n",
    "            elif exid==\"utterance\":\n",
    "                repeat = len(intensities)\n",
    "            for _ in range(repeat):\n",
    "                if _==0:\n",
    "                    labeltext = fn\n",
    "                else:\n",
    "                    labeltext = \"\"\n",
    "                text += f\"\"\"\n",
    "                          <th scope=\"col\">{labeltext}</th>\n",
    "        \"\"\"[1:]\n",
    "        text += \"\"\"\n",
    "                        </tr>\n",
    "                      </thead>\n",
    "        \"\"\"[1:]\n",
    "\n",
    "        text += \"\"\"\n",
    "                      <tbody>\n",
    "                        <tr>\n",
    "                          <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">Text</th>\n",
    "        \"\"\"[1:]\n",
    "        for fn in filenames_dir[exid]:\n",
    "            if exid==\"expressiveness\":\n",
    "                labeltext = f\"<b>Text</b>: {transcriptions[fn]}<br>\"\n",
    "                text += f\"\"\"\n",
    "                          <td>\n",
    "                            <p>{labeltext}</p>\n",
    "                          </td>\n",
    "        \"\"\"[1:]\n",
    "            else: \n",
    "                for intensity in intensities:\n",
    "                    if exid==\"utterance\":\n",
    "                        labeltext = f\"<b>Text</b>: {transcriptions[fn]}<br><span style='color:{emotioncolors[emotion]};'><b>Intensity: {intensity}</b></span>\"\n",
    "                    elif exid==\"word\":\n",
    "                        # labeltext = f\"<b>Text</b>: {transcriptions[fn]}<br><span style='color:{emotioncolors[emotion]};'><b>Intensity: {intensity}</b></span>\"\n",
    "                        labeltext = f\"<b>Text</b>: {modifiedparts[transcriptions[fn]]}<br><span style='color:{emotioncolors[emotion]};'><b>Intensity: {intensity}</b></span>\"\n",
    "                    text += f\"\"\"\n",
    "                          <td>\n",
    "                            <p>{labeltext}</p>\n",
    "                          </td>\n",
    "        \"\"\"[1:]\n",
    "        text += \"\"\"\n",
    "                        </tr>\n",
    "        \"\"\"[1:]\n",
    "        \n",
    "        for key in experiments[exid]:\n",
    "            text += f\"\"\"\n",
    "                        <tr>\n",
    "                          <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">{experiments[exid][key]}</th>\n",
    "        \"\"\"[1:]\n",
    "            for fn in filenames_dir[exid]:\n",
    "                if exid==\"expressiveness\":\n",
    "                    a = fn.split(\"_\")\n",
    "                    d1, d2 = a[0], a[1]\n",
    "                    wavbasefile = base_dir + f\"{key}/dev-clean/{d1}/{d2}/{fn}.wav\"\n",
    "                    if os.path.exists(wavbasefile):\n",
    "                        wavfile = wavbasefile\n",
    "                    else:\n",
    "                        a = wavbasefile[:-4] + \"-0.wav\"\n",
    "                        if os.path.exists(a):\n",
    "                            wavfile = a\n",
    "                    wavfiles += [wavfile]\n",
    "                    text = text + wavfiletext(wavfile)\n",
    "                else:\n",
    "                    spk = fn.split(\"_\")[0]\n",
    "                    keyname = key.split(\"---\")[0]\n",
    "                    if \"ground-truth\" in key:\n",
    "                        wavbasefile = base_dir + f\"{keyname}/{spk}/{emotions[fn]}/evaluation/{fn}.wav\"\n",
    "                    else:\n",
    "                        wavbasefile = base_dir + f\"{keyname}/{spk}/{emotions[fn]}/evaluation/{fn}\"\n",
    "                    for intensity in intensities:\n",
    "                        if os.path.exists(wavbasefile):\n",
    "                            wavfile = wavbasefile\n",
    "                        else:\n",
    "                            if \"---cut\" in key:\n",
    "                                wavfile = wavbasefile + f\"---{emotion}---{intensity}___split.wav\"\n",
    "                            else:\n",
    "                                wavfile = wavbasefile + f\"---{emotion}---{intensity}.wav\"\n",
    "                        if os.path.exists(wavfile):\n",
    "                            wavfiles += [wavfile]\n",
    "                            text = text + wavfiletext(wavfile)\n",
    "                        else:\n",
    "                            text += \"\"\"\n",
    "                          <td>\n",
    "                              <p></p>\n",
    "                          </td>\n",
    "        \"\"\"[1:]\n",
    "            text += \"\"\"\n",
    "                        </tr>\n",
    "        \"\"\"[1:]\n",
    "            \n",
    "        text += \"\"\"\n",
    "                      </tbody>\n",
    "                    </table>\n",
    "                  </div>\n",
    "                  <p class=\"lead\">* please scroll horizontally to explore additional columns in the table.</p>\n",
    "        \"\"\"[1:]\n",
    "    text += \"\"\"\n",
    "                </div>\n",
    "    \"\"\"[1:]\n",
    "    # extexts[exid] = headerfn + headertext + body + tableclosure\n",
    "    # extexts[exid] = headerfn + headertext + text\n",
    "    extexts[exid] = text\n",
    "    extexts[exid] = \"\\n\".join([a[4:] for a in extexts[exid].split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholetext = \"\"\n",
    "wholetext += initial\n",
    "# for exid in experiments:\n",
    "for exid in list(experiments.keys()):\n",
    "    wholetext += extexts[exid]\n",
    "wholetext += closure\n",
    "f = open(\"index.html\", \"w\")\n",
    "f.write(wholetext)\n",
    "f.close()\n",
    "print(wholetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# files = glob.glob(\"./audio/control/*/*/*/*/evaluation/*\")\n",
    "# files.sort()\n",
    "# delete_files = []\n",
    "# save_files = []\n",
    "# for path in files:\n",
    "#     if np.array([fn in path for fn in controlfns]).sum()>0:\n",
    "#         save_files += [path]\n",
    "#     else:\n",
    "#         delete_files += [path]\n",
    "# for path in delete_files:\n",
    "#     os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# wavfiles = list(set(wavfiles))\n",
    "# wavfiles.sort()\n",
    "\n",
    "# copy = False\n",
    "# tgtdir = \"./audiosamples/\"\n",
    "# for path in wavfiles:\n",
    "#     savepath = tgtdir + path[len(base_dir):]\n",
    "#     if copy:\n",
    "#         os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "#         shutil.copy(path, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
